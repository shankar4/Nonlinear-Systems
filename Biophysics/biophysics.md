I provide here relevant summaries from certain books and papers. Summaries are found [here](https://github.com/iShankar/Atherosclerosis/blob/master/biophysics/BialekBookSummary.md).

Bialek's book: A 2011 draft is available [here](https://www.princeton.edu/~wbialek/PHY562/WB_biophysics110918.pdf). He uses photon counting in vision to showcase the opportunities for quantiatvie, physics-style experiments to understand biological phenomena. He develops three general physical principles to study living systems: (1) Extraction of low level signal from noise, and consequent energy minimization; (2) Irrevelance of finely tuned parameters in understanding their extraordinary performance ; and (3) Critical role of the representation and flow of information. 

Ch. 1 Intro: In physics, we (try to) teach principles and derived the predictions for particular examples. In biology, teaching proceeds (mostly) from example to example, system to system. As physicists studying the phenomena of life, do not search for a new force of nature; rather understand how the same force that causes carbon-based materials to look like rocks or sludge cn, under some conditions, but also cause reorganize itself to be animate.  Looking back, many giants of classical physics routinely crossed borders among disciplines such as physics, chemistry, biology, and psychology. Some wished to test the universality of physical laws (eg., energy conservation); others tried to understand the sensory data in biology. .... Schrodinger ( in his book: "What is Life?") Outlined how nonequilibrium systems can generate structure out of disorder, continuously dissipating energy (*my comment: low amount?*). Reductionist view of molecular biology has shown that many of the basic molecules of life are universal, shared across organisms via evolutionary and adaptation influences. Classical biology emphasized the complexity and diversity of life. "Structure Determines Function" was clarified once 3-D protein and DNA structures were identified. He gives many examples (pg. 11) of how advances in physics helped explore new focused biological subdomains (eg., receptors and ion channels in neural info flow). Physics has developed experimental techniques that allow fuller exploration of biological questions. By focusing on methods, physicists and biologists tend to ask different questions faced with the same biological phenomena. *Early Intellectual leaders postulated that qualitative phenomena of life could hold clues for deep theoretical insights, that there should be an over-arching physics of life. His book emphasizes this view*. Looking forward: Biological questions are answered by experimental discovery, unlike physics where theory and experiments go hand-in-hand. In Physics, a set of general theoretical principles, all interconnected, define the possible based on many earlier quantitative experiments and predict new phenomena which may then be experimentally verified. A predictive power of similar type is hoped for in physics of biological systems.... Despite much progress in identifying microscopic mechanisms, there has not been much progress in elucidating  the "order parameters" that characterize the macroscopic state. A principle that differentiates the genuinely biological parts of parameter space from the rest can be elevated to a theory and used to predict similar phenomena in other living systems. **If biological systems occupy only a small region of the parameter space, one needs to understand the dynamics (adaptation, learning, and evolution)  by which parameters arrive at these special values.** To survive in the world, organisms solve a variety of problems: energy conversion, weak signal sensing, control of complex dynamical systems, reliable information transmission across generations and regions, rate control of thermally activated processes, and prediction of multi-dimensional trajectories, etc. .. Physics places limits on what is possible, allowing us to evaluate performance of a biological system on an absolute scale. *It makes precise our intuition that organisms are really good at solving some very difficult problems.* His approach in his Princeton graduate course: Start out with a specific well studied and quantified biological phenomena that has microscopic to macroscopic physics perspectives, and extraction of generalizable principles. He chose the visual system that encompasses quantum dynamics of single (photon) molecules to the macroscopic dynamics of human cognition.    

Ch. 2 First Look: Using the example on photon counting in vision. Three researchers in an earlier era experiment sat in a dark room and were subject to flashes of different intensities; their proabiity of observance of a flash are plotted on a log x scale (x is the light intensity at the cornea). It is known that the flash follows a Poission distribution (if λ is the average number of events per interval, then the probability density function peaks when K , the number of photons in any given interval,  equals λ; put another way, the cumulative distribution function has a value of 0.5 at this K value. K can take values of 0, 1, 2, etc.). See [Wiki](https://en.wikipedia.org/wiki/Poisson_distribution) article. Figs 1 and 2 in the book plots this data for these three 'subjects', presumably with different α values (due to differences in age related corneal diffractions and scattering); and Fig 2 shows a good fit with Poisson distribution for the same K =6 for all three. *The probabilistic nature of our perceptions is determined by the physics of photon counting*. A good fit was found for K= 5 to 7; consider that the retina has 500 receptor cells for a flash projected onto the retina (a retina has a total of 120 million rods and 6 million cones). So, sensitivity to a single photon means that each cell is capable of responding to a single molecular event. *Averaging across individuals with different αs will not match the Poission distribution - leading us to believe that the visual system does not count single photons*. **Averaging across an ensemble of individuals can be qualitatively misleading**. There is much to learn from old experiments...Why K of ~6? there must be a noise source in the visual system that is equivalent to counting ~10 photons over the time window (~0.1 s) and retinal sub-area. He calculates the noise (thermal iosmerization noise) as < 0.1 photon/receptor cell per second or less. People can adjust the threshold K in different situations. *All sensory tasks involve a discrimination between signal and noise, and hence different strategies to trade off different kinds of errors*. By experimenting with toads, it has been shown that this is an upperbound for noise which can be lowered with a **temperature drop**. 

Dynamics of biochemical networks: Currents in biological systems are carried not by electrons and holes, as in semiconductors, but **by ions (largely Sodium ions) moving through water**. the cell membrane is not very selective to one specific type if ion. In a macaque monkey, red cones have a peak sensitivity at 570 nm; at this wavelength, blue cones have sensitivities that are 10<sup>5</sup> lower. *These cells make that much less protein, since all cells have the same DNA template*. Fig 22 shows the cascade leading from photon absorption to ionic current flow in rods. Rhodospin is activated by photon, converts Transducin (a moleculree of a broad class of G-proteins that couple receptsors to enzymes) to its activated form, which converts PDE, an enzyme phosphodiesterase, which degrades cGMP to GMP, which opens the a cellular channel for ion current. GC (Gyanylate Cyclase) synthesizes cGMP from GTP (guanosine triphosphate). *Universality of these components meants that understanding this can provide a model for understanding an  enormous range of biological processes.* These channels of pores are large molecules (proteins) embedded in the cell membrane, and respond to the electric field or voltage across the membrane and also to small molecular bindings. *The coupled dynamics of channels and voltage turns each cell into a potentially complex nonlinear dynamical system*. Ions and small molecules diffuse freely through water, not a cell with a double membrane. Water is polar and screens the charge (unilke the cellular membranes). 
(In physics, screening is the damping of electric fields caused by the presence of mobile charge carriers [Wiki](https://en.wikipedia.org/wiki/Electric-field_screening)). An Ion channel (a large protein molecule) serves  as a hole in the membrane. ionic flux = *cv*, where c is the ionic concentration (~100 mM or 6 x 10 <sup>19</sup> ions/cm<sup>3</sup>) amd v is the velocity. *v = μF, where μ is the ion mobility and F is the electric field force = q<sub>ion</sub> E, where q is the charge and E is the electric field strength = V/l, where V is the voltage across the membrane of thickness l*. q<sub>ion</sub> = 1.6 x <sup>-19</sup> C. Assume the channel diameter = diameter of a single ion (~0.3 nm). L ~ 5 nm. This yields conductance of an open channel as ~20 pS. With V = 50 mV, the current due to opening a single channel is 1 pA. However, *this does not yield a square edge current trace*. These channels open and close rapidly, so on the slow scale of rod response, one sees a **graded current** proportional to the probability of the channel being open. The current produced by the population of channnels in the rod cell membrane depends continuously on the concentration of cGMP. The effect of this fast and flickering channel noise is much reduced at the lower time scale (of 0.1s) of the rod. **Evolution has selected for very fast channels to be present in a cell that signals very slowly!** Our genome codes for hundreds of different types of channels which may differ in kinetics, with faster channels responding to the more rapidly varying signals. Pipette experiments show that 3 molecules of cGMP need to bind to the channel to open it. **"Cooperativity and allostery are important themes in biochemical signaling**. The current can range 3-fold as cGMP is varied 2 fold. There is a feedback mechanism in the rod, where Ca enters through the open channels, binds, and inhibits the activity of GC. 

The first synapse and beyond: How does the retina add up the responses of the many rods so that the observer can reach a decision? Need to integrate over all the receptors in that area covered by the flash. This involves spatial summation of currents with a mean current of ~ 1 pA, with continuous background noise of ~0.1 pA. This S/N ratio is OK in a single cell, buut there will be a problem for our integration. With N<sub>cells</sub> = 500, we will have an effective additive noise of ~2.2 pA - that means confusing 3 to 5 photons with a blank. This problem is even more serious for larger areas. *Summing the signals from many cells buries tthe clear single photon response under the combined noise*. Could not be! Each rod's signal is passed through a **sigmoid (strong nonlinearity)** so the output is only a 1 or 0 and then pooling their outputs. Contrast this with neuronal transmission: it adds up the inputs and then passed their sume through a nonlinearity. His theoretical calculations (*yet to read through) match with experimental results*. Also, note that in addition to amplitude, signal and noise have different frequency bands. Thus the rod/bipolar synaps implements an optimal filter. There are two types of noise in a rod cell: Spontaneouls isomerizations of rodospin, which have the same frequency content as the real signal, and the continuous background noise, which extends to higher frequency. To extract the signal from isomerization signal, one needs a high pass filter. This unsmoothing is cut off by the background noise and the result is a bandpass filter. This also speeds up the rather slow response of the rod. 

Perspectives: This material is similar to **Chapter 3 of the published book**. (1) There is at least one example of a real biological system that is amenable to reproducible, quantitative experiments of physics; Experimentalists need to get right many things to achieve this. (2) Performance of these complex biological systems is determined by the physics of the problem that the system is choosen to solve. (3) We can observe measurable parameters such as ion currrents, filtering, and nonlinearities in other domains also. What are we doing when we look for an 'explanation' of the data? We could jump to the microscopic level and explain, but this may not be satisfying and may not relate to phenomenological parameters, when the goal is to abstract principles. **A second perspective**, developed here (with the example of photon counting) is that the system has evolved as the best solution to a problem that is essential in the life of the organism. This does not tell how the optimum is reached, but can predict the observable behavior of the system. We should discipline ourselves and insist on **criteria for such optimization**, not philosophical discussions, that allow such claims to be meaningful and predictive. We need to have a theory which defines the physical limits to performance. We should be able to measure this performance metric and compare its value with the theoretical maximum. also, maximizing this metric should generate some definite predictions about the structure and dynamics of the system, predictions that can be tested in independent, quantiative experiments. In chapters 4 to 6 ( numbering from the published book, not this abstract), he discusses three broad ideas about what these generalizable metrics might be. 

Chapter on "Noise is Not Negligible" (Chapter 4 in the published book): The first example is on the retina. See Chapter 3 notes [here](https://github.com/iShankar/Atherosclerosis/blob/master/biophysics/BialekBookSummary.md) for more on that. Here are some notes on the inner ear: Threshold of hearing at the inner ear in power is ~ 4 x 10<sup>-19</sup> W. This suggests a bandwidth of 100 Hz to be sure the signals are above thermal noise. Several different ways (neuronal level, human auditory level, etc) indicate that this to be true. Extrapolation of light microscopy data indicates a basilar movement of 0.1 nm at 0 dB SPL, an extremely small displacement. The peak of the vibrations to a single frequency spreads to more than ten times the apparent bandwidth over which we integrate. This suggests a later mechanism to sharpen frequency selectivity, perhaps similar to lateral inhibition in the retina. **Perspectives**: Many life phenomena show impressive degree of reliability and precision. Organisms reproduce and develop with high predictability, and our sensory experiences feel certain and solid. But individual cells look very noisy. How to understand the system level behavior from this mess. Such systems are normally described in terms of sensing, processing, and responding (of signals). In a lab setup, if there is a high gain in one stage, one will see high voltage fluctuations; There could be also high amount of noise with no gain, as with common mode noise. We refer noise to input, so we can determine the minimum detectable signal level. He finds that in many disparate systems, the noise performance of biological systems is indeed close to the relevant physical limits. Even when the system has control to use higher levels (as with higher concentration of TFs), this physics limit is utilized, perhaps because these molecules are costly. At another level, there is kinetic proofreading, where the accurate synthesis of proteins costs energy; so, there must be a balance between the cost of errors and that of correcting them. Although not new, these concepts can now be monitored at lower levels, thanks to instrumentation advances. He feels that the most general and compelling formulation of the problem is still missing. Active filtering and kineting proofreading are ways of reducing noise levels by spending energy to maintain a **nonequilibrium state**. Real-world problems of biological systems are complex and multifaceted...

Chapter on "No Fine Tuning" (Chapter 5 in the published book): A system can be described with a large number of simultaneous differential equations (my cardiovascular simulation involved 100 such diff equations). If parameters are chosen randomly, the system may become chaotic. This is not what characterizes life. However, it is a stretch to postulate that all parameters have been set to just the right values. Reason: the cells, that collectively lead to these parameters, do not have control over the environment (temperature, pH, etc). Either the copy number of proteins made is not important or the cells can finely control the copy number. **Robustness vs fine tuning**. If the model is too sensitive to parameter variations, it is not satisfying - we are missing something in our mathematical formulation. The qualitative debate about robustness vs fine tuning is more quantitative at the level of single protein molecule. Monomers of a protein polymer are chosen from a set of 20 AAs. In a typical protein 200 AA long, there are 20<sup>200</sup> possible combinations (or 10<sup>260</sup>). The human might have chosen a few tens of thousands, thus a small fraction of the possible sequences. Building blocks of protein structures are the secondary structures α-helices and β-sheets.  Pauling and Corey's theoretical proposal (based on repetitive hydrogen bonding facilitated with these foldings) was confirmed with X-ray cryystallography. Individual propensities of the AA decide this folding, but also the environment (as the structure is densely packed). Sequence obviously matters; but it can't be that it has to be the exact sequence. It has been shown that *many AA substitutions do not alter the structure or function of the protein*. Other quantitative modulations could be useful for other related organisms or differing environments. In Problem 87, he takes the example of a *simple salt solution* (with Na+ and Cl- ions) to show that even the electrostatic interactions are effectively local. If the interaction V=0, the polymer will self-avoid; if V=-U, there is a net attraction that causes the polymer to collapse and become more compact. if V also depends on the AA identities, then there is more randomness. He uses the example of random interactions in the spin glass. Taking the example of 3 spin bodies (magnetic dipoles, that point up or down), he shows (Fig. 5.4) that no matter what configuration of spins we choose, one of the bonds is always unsatisfied (**"frustrated spins"**). That is, low energy states are nearly degenerate, or different low-lying states correspond to very different spin configurations. *Randomly chosen proteins will not, in general, have unique ground-state structures. There will be many inequivalent structures with nearly the same low energy, separated by large barriers.* **This contrasts sharply with the ability of real proteins to fold into partciular, compact conformations that are likely determined by the sequence**. Conclusion: Proteins occuring in nature are not typical of random sequences. At the same time, not every detail of the AA seq can be important. *Description of life cannot depend on fine tuning, but neither are the phenomena of life generic*. Evolutionary pressure and physical principles are both probably causative in deciding on the actual AA sequences. A candidate principle for selecting functional sequences is the minimization of the frustration. Theory and experiments are in aggreement, at least for small proteins. Mutations show that many sequences map into the same structure. Inverse folding problem: Given a compact structure, can we find the sequence? To lower the energy, *put the polar residues on the outside and the hydrophobic residues on the inside*. Proteases digest selective proteins. In Fig 5.12, he shows structures of a mammalian enzyme chymotrypsin and a bacterial enzyme SGPA. Their structures are similar, though with only 25% overlap in AA sequence. Fig. 5.14 shows that artificial (or random) WW domains that respect pairwise sequence correlations approximate the properties of naturally occuring sequences. *Statistical mechanics*: Entropy is a measure of randomness or disorder. Fig. 5.20 gives the maximum entropy distribution of sequences. It gives the probability that a particular sequence will occur. *Correlations can occur over longer distances relative to the underlying interactions*. Fig. 5.15 shows that one can read off the physical contacts between AAs (on a pair of receptors and associated signal proteins) and infer the 3-D structure of the proteins. *P. 273: the number of TFs is very much smaller than the # of genes. This means that there must be many examples within the same organism of binding sites for each TF*. P. 275: Ability to identify functional sequence elements simply from their statistics is evidence that, in the continuum from randomness to fine tuning, real biological systems are not at the random limit.  **Perspectives**: Robustness vs fine-tuning. He takes examples at three levels: (a) AA sequence: There is a natural class of molecules that can be built from the same monomers but with different sequences. For a biochemical or genetic network, one may wish to generalize to a class of networks that has the same topology but different parameters on nodes and links. (b) The ion channels in a single neuron provide a simple example of this. (c) For networks of neurons, the plasticity of the strengths of synaptic connections during learning leads us to think of networks with similar topology of connections with different strengths. In the extreme, robustness means that functional behavior is largely invariant over the whole class of networks. If so, we should be able to choose networks at random and have them function. For biochemical and genetic networks, chaos seems less generic, but topology of the network must be chosen carefully to obtain functional behavior without adjusting parameters. In neural, sensory inputs serve to drive the network out of chaos to functional or ordered state. Why is chaos not common in other networks?  In most systems he studied, chosen parameters do not cause functional behavior. There is some tuning. In single neurons, number of copies of different channels is a form of physiological adaptation, connecting *electrical activity, intracellular messengers, and gene expression control*. In neural networks, strengths of the synapses are adjusted during learning, and in some processes, learning happens all the time. Eg., the behavior that system is trying to stabilize is very ar from typical in the space of possible networks. For AA and DNA sequences, the adjustment to functional behavior occurs in evolutionary time scales. **We can think of adaptation, learning, and evolution as different mechanisms of accomplishing the same task, albeit on different time scales and time periods**. *Robustness has been advanced because 'living organisms simply cannot adjust parameters accurately enough to guarantee reliable, reproducible functions. This is wrong* Cells can and do exercise precise control over the number of molecules that they make, so that absolute concentrations of relevant molecules *can be* reproducible from cell to cell. Also, the ion channel example shows that, in the natural parameter space for the cell, there are many ways of achieving the same function; there is no reason for precise control. This does not imply sloppiness, but rather of a *many-to-one mapping* from microscopic parameters to macroscopic functional properties.  This, however, is necessary, but not sufficient for the organism to survive under varying environmental conditions, both inside and oustide. Chemical reaction rates vary, often by a factor of 10 or more; there is not guarantee that different rates in a given network will scale together. Eg. circadian rhythm is invariant to temperature changes. I found papers that discuss this at the genetic (1999) and ionic channel (2016) level. However, environmental diversity has led to species differentiation. Yet simple lab experiments show that many aspects of life are nearly invariant over a wide range of environmental conditions, much wider than we might expect from simple models.

Locating life in this spectrum between precisely controlled (rather than finely tuned) dynamics and generic or robust behavior is important. We need to understand from experiments *which features are robust against such variations, and we need evidence that organisms actually face these variations in their natural environment*. On the theoretical side, we need more anchor points, such as the *random heteropolymer and the random neural network*.  We also need a statistical mechanics of systems with random parameters that allows for parameters with nontrivial distributions. These are substantial challenges. 

 is theChapter on "Efficient Representation"(Chapter 6 in the book): About flow of info. Entropy of a gas measures our lack of info about the microscopic state of the molecules. Shannon provided a quantifiable concept: Entropy is the unique measure of available information consistent with certain simple and plausible requirements. It also answers the pracical question of how many intermediate signals or states need to be noted. He hypothesizes here that biological systems form efficient representation, maximizing the amount of info that they transmit and process, subject to fundamental physical constraints. *The goal of learning is to build an efficient representation of what we have seen*. He takes the example of two friends, M and A. M asks A his opinion on a political event. M knows A well. M can make a list of A's possible responses and assign probabilities. One can compute an entropy S, much like that for gas in statistical mechanics. Entropy measures M's uncertainty (or lack of knowledge) about A's response. Once A responds, all the uncertainty is removed and Entropy is reduced to zero. Shannon showed that this reduction in uncertainty is equal to the information M gains by getting A's response. Information gained is a function of these probabilities. Find the function. When measuring info, it is conventional to choose base two (since most basic answers are Y/N). Thus we *f(N) = log(sub>2</sub>N*. Units would be bits. One bit is the info contained in the choice between two equally likely alternatives. *I for all p<sub>n</sub> = Sum over all n of p<sub>n</sub> log<sub>2</sub> p<sub>n</sub>*. If all possible answers have the same probability and N = 2<sup>m</sup>, then the entropy is just m bits. In our case, entropy is emerging as the answer to two very different questions: (1) quantify the concept of information gained when the answer is heard; (2) represent it in the smallest possible space. If one has many unlikely answers (vs fewer more likely answers), to ensure tht long sequences of answers take up as little space as possible, use *l<sub>n</sub> = ~ log<sub>2</sub> p<sub>n</sub>* bits to represent each individual answer. *His example* (pp. 362): p1 = 1/2, p2= 1/4, p3=1/8, p4=1/8. Say we use two bits (4 possibilities). Then the entropy is = 1/2* log2 + 1/4* log4 + 2/8* log8 (it is not log2, but log<sub>2</sub>2 - just lazy! - md requires additional tags) = 7/4 (as it must be?). Say, we now represent the four possibilities with expanding code: 0 for 1, 10 for 2, 110 for 3 and 111 for 4. *This code does guarantee the uniqueness of the code words*. The length of each code word obeys the rule *l<sub>n</sub> = ~ log<sub>2</sub> p<sub>n</sub>* bits. So, on the average, the number of binary digits used per answer = the entropy (1/2 * 1 + 1/4 * 1 + 2/8 * 2 = 1.25 < 2 bits with a non-expanding code). *Thus entropy is both the amount of info gained and the amount of space needed for the information*. To reach this maximally compact representation, at least implicitly use the pdf to determine the length of each code word. Problem 129, pp. 364: if *p<sub>A</sub> << p<sub>B</sub>*, then the entropy = ~ *p<sub>A</sub> log<sub>2</sub> (e/*p<sub>A</sub>)* where e is the base of natural algorithms (2.7). The first term will get closer to zero, as the info content approaches zero. So, the second term dominates.  Say *p<sub>A</sub>* is 1/16, then entropy is ~ 0.34 (vs 0.25 for the second term only). He suggets for saving space this: Use 1111 for A and code blocks of B by the length of the B's run. Need some way of distinguishing when the run = 15. *His question: Does this code come close to the lower bound on code length set by the entropy?* It does for this example. The connection between entropy and information has one consequence: correlations or order reduce the capacity to transmit info. For four letter English words, the entropy for random letters would be 4 log<sub>2</sub>26 = 18.8 bits. In the collected works of Jane Austen, the one-body correlation, which measures unequal frequencies with which letters are used, reduced this to 14 bits. With 2-body correlations, it is cut to 7.48 bits. *Correlations have the consequence of reducing our vocabulary and hence our capacity to transmit information*. *Noise and information flow*: Figure 6.9 shows spatial profiles of Hunchback (Hb) expression in the early *Drosophila* expression. The variance (noise) at each position is about 0.1 units vs the maximum mean expression level of 1. Entropy calculations show that the expression level of Hb protein provides 2.26 +/- 0.04 bits of positional information (my simplistic and quick quess: five points to define the sinusoid shape - the zero crossings, the peak and trough, and the end point - that is, 5 positions which requires 2.32 bits - log <sub>2</sub>5). In the Gaussian distribution this is a lower bound on the info, but it is large enough to show that the bound is tight. Classically, the gap genes have been described in terms of on/off patches (or binary - 1 bit). **That would miss the story**. *Does Biology care about Bits*: He connects first info to gambling and then to info-biological function-evolutionary fitness. Say you put (consistently) a fraction, f on heads (then 1-f will be on tails) for every toss, then results of ith flip will be 2f or 2(1-f) depending on the result. After N successive flips, the gain is a product of these individual flips. As N tends to infinity, the dependence on flips goes away. (1/N) * sum of n<sub>i</sub> over all i = 1 to n tends to P, where n<sub>i</sub> = 1 with probability *p* and 0 with probability 1-*p*. To maximize the winnings' growth, he shows that the optimum fraction, *f<sub>opt</sub> = p*. Further, the maximum rate the winnings will grow (in this case) is equal to the information that you have about the outcome of a single coin flip. This analysis is from Kelly (1950s) - the *maximum rate of winnings' growth is the information*. A simple biological analogy: bacteria can grow slowly and outlast the antibiotic or overwhelm the antibiotic by rapid growth. If bacteria gains information about the hostile (antibiotic) information, then it can increases its 'winnings.' This is called *phenotype switching*. Another example is the bacterial growth which metabolizes the substrate to produce a growth enzyme (or a man/woman on a deserted island with a limited supply of food at the start). Organisms can grow faster if they gather and represent more information, but this is not guaranteed. - they might make poor choices and end dead. pp. 408: Similar to rate-distrortion curves the information theory literature. Eg. If we measure image quality by a complex metric, then to have an image of a certain quality, one needs to transmit a minimum number of bits. In rate-distortion curve, some bits are defined as being more relevant vs others (eg. may be some specific environment variables over others). This needs extra bits in the right places. One may gather the same number of bits in a variety of combinations, but not be optimized to survive. *Bits are necessary but not sufficient* - judicical combination is needed. He also shows that the time it takes to find something is bounded by the uncertainty (as measured by entropy) about its location (pp. 409). Thus to search efficiently, the only way to speed things up is to lower the entropy, or equivalently, to gain information (to reduce uncertainty). *My comments*: This is the same as "Knowledge is power." More certain the future info, the higher the success rate. Similar to taking a known route to drive to work vs experimenting with newer paths. Considering stochastics, it appears that ratios of expressed intermediaries is more important than the absolute levels. 

**Perspectives**: Some bits are more useful than others and there is challenge of acting rather than just collecting data. This suggests new kinds of experiments. In some systems these experiments have generated interesting results. He takes an example of regulation of gene expresion: *lac* operon in *E.Coli*. We need to know the distribution of lactose concentrations encountered by these cells in their natural environments. Khan academy has a good discussion [here](https://www.khanacademy.org/science/biology/gene-regulation/gene-regulation-in-bacteria/a/the-lac-operon). It is known that the total # of *lac* repressor proteins is small, but their dynamic range is not known. After reading the discussion at Khan, the more appropriate questions should be about the dynamic range of CAP which must be bound to the DNA to promote transcription. When glucose levels are low, cAMP is produced which attaches to CAP, allowing it to bind DNA. But his point is still valid: "Vastly more is known about the details of the DNA sequences that are targeted by TFs involved in the regulation of metabolic genes than is known about the real-world variations in nutrient conditions that create the need for metabolic regulation." Ethologists study systems specialized for the processing of particlular sense data, such as bird song. They emphasize the natural context. At least some aspects of neural circuitry are arranged to generate efficient representation of incoming data. **Context matters**. Experimental or lab studies have limited to a few alternatives. As the technology for monitoring behavior improves, one can ask whether the continuous variations in natural behaviors are just noise or **are related to goals and context**. We are bombarded by complex data, but our behaviors are relatively limited, one might think. But our sensory inputs are highly structured, as they derive from a set of limited set of causes and effects in the environment. They carry much less info than is presumed. Our receptors provide limited noisy view, further reducing the bandwidth. However, our motor outputs are rich. Is there a careful orchestra of environment-sensor-motor integration that essentially makes use of a large amount of informatin available about this environment? Needs to be explored. Another aspect: bits have value. Some bits are useful. Separating predictive info from the background of nonproductive clutter is a biologically relevant challenge. Within this task, there is are special cases of **signal processing to learning, problems that we usually think of as belonging to a different level of biological organization with very different mechanisms**. 

References:

1. Bialek, W., Biophysics: Searching for Principles, Princeton University Press, Princeton, 2012
2. Snell, F.M., Shulman, S., Spencer, R.P., and Moos, C., Biophysical Principles of Structure and Function, Addison-Wesley, Palo Alto, 1965
3. A 1976 book
4. Tuszynski, J.A., and Kurzynski, M.., Introduction to Molecular Biophysics, CRC Press, Boca Raton, 2003
